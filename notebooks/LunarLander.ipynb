{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chpppy/Colab/blob/main/notebooks/LunarLander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXilmjqHxLtW",
        "outputId": "a6e5c164-ac92-47db-eadd-c6a144a0d0cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/958.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.3/958.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n",
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-2.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (1.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.5.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0,>=2.3->stable_baselines3) (12.8.61)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable_baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (4.55.6)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable_baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.5.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: stable_baselines3\n",
            "Successfully installed stable_baselines3-2.5.0\n",
            "Collecting sb3-contrib\n",
            "  Downloading sb3_contrib-2.5.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: stable_baselines3<3.0,>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from sb3-contrib) (2.5.0)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3<3.0,>=2.5.0->sb3-contrib) (1.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3<3.0,>=2.5.0->sb3-contrib) (1.26.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3<3.0,>=2.5.0->sb3-contrib) (2.5.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable_baselines3<3.0,>=2.5.0->sb3-contrib) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable_baselines3<3.0,>=2.5.0->sb3-contrib) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable_baselines3<3.0,>=2.5.0->sb3-contrib) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0,>=2.3->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (12.8.61)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (4.55.6)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable_baselines3<3.0,>=2.5.0->sb3-contrib) (3.0.2)\n",
            "Downloading sb3_contrib-2.5.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.8/92.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sb3-contrib\n",
            "Successfully installed sb3-contrib-2.5.0\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.37)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.9 alembic-1.14.1 colorlog-6.9.0 optuna-4.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install stable_baselines3\n",
        "!pip install sb3-contrib\n",
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install gymnasium[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1n1kgK0zSAI",
        "outputId": "b8ae857b-750c-4e54-83ea-04d274ff28cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.3.0\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379449 sha256=6d7aec7897bbd5d3917c88bc468461e4d737468414eff24838f571bda1031e72\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V0zKH_ZexLtY"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from stable_baselines3 import PPO, A2C\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ub-ovNSlxLtY"
      },
      "outputs": [],
      "source": [
        "# config\n",
        "N_TRIALS = 100  # Maximum number of trials\n",
        "N_JOBS = 1 # Number of jobs to run in parallel\n",
        "N_STARTUP_TRIALS = 5  # Stop random sampling after N_STARTUP_TRIALS\n",
        "N_EVALUATIONS = 2  # Number of evaluations during the training\n",
        "N_TIMESTEPS = int(2e4)  # Training budget\n",
        "EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
        "N_EVAL_ENVS = 5\n",
        "N_EVAL_EPISODES = 10\n",
        "TIMEOUT = int(60 * 60)  # 15 minutes\n",
        "\n",
        "ENV_ID = \"LunarLander-v3\"\n",
        "\n",
        "DEFAULT_HYPERPARAMS = {\n",
        "    \"policy\": \"MlpPolicy\",\n",
        "    \"env\": ENV_ID,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "piUC4pqixLtZ"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from optuna.pruners import MedianPruner\n",
        "from optuna.samplers import TPESampler\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "pWuqqfWRxLtZ"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def sample_ppo_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Sampler for PPO hyperparameters.\n",
        "\n",
        "    :param trial: Optuna trial object\n",
        "    :return: The sampled hyperparameters for the given trial.\n",
        "    \"\"\"\n",
        "    # Discount factor between 0.9 and 0.9999\n",
        "    gamma = 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True)\n",
        "    max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5.0, log=True)\n",
        "    # 8, 16, 32, ... 1024\n",
        "    n_steps = 2 ** trial.suggest_int(\"exponent_n_steps\", 9, 10)\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    # TODO:\n",
        "    # - define the learning rate search space [1e-5, 1] (log) -> `suggest_float`\n",
        "    # - define the network architecture search space [\"tiny\", \"small\"] -> `suggest_categorical`\n",
        "    # - define the activation function search space [\"tanh\", \"relu\"]\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1, log=True)\n",
        "    net_arch = trial.suggest_categorical(\"net_arch\", [\"tiny\", \"small\"])\n",
        "    activation_fn = trial.suggest_categorical(\"activation_fn\", [\"tanh\", \"relu\"])\n",
        "\n",
        "    ### END OF YOUR CODE\n",
        "\n",
        "    # Display true values\n",
        "    trial.set_user_attr(\"gamma_\", gamma)\n",
        "    trial.set_user_attr(\"n_steps\", n_steps)\n",
        "\n",
        "    if net_arch == \"tiny\":\n",
        "        net_arch = {\"pi\": [64], \"vf\": [64]}\n",
        "    else:\n",
        "        net_arch = {\"pi\": [64, 64], \"vf\": [64, 64]}\n",
        "\n",
        "\n",
        "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU}[activation_fn]\n",
        "\n",
        "    return {\n",
        "        \"n_steps\": n_steps,\n",
        "        \"gamma\": gamma,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"max_grad_norm\": max_grad_norm,\n",
        "        \"policy_kwargs\": {\n",
        "            \"net_arch\": net_arch,\n",
        "            \"activation_fn\": activation_fn,\n",
        "        },\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "vBK8IRdnxLtZ"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "\n",
        "class TrialEvalCallback(EvalCallback):\n",
        "    \"\"\"\n",
        "    Callback used for evaluating and reporting a trial.\n",
        "\n",
        "    :param eval_env: Evaluation environement\n",
        "    :param trial: Optuna trial object\n",
        "    :param n_eval_episodes: Number of evaluation episodes\n",
        "    :param eval_freq:   Evaluate the agent every ``eval_freq`` call of the callback.\n",
        "    :param deterministic: Whether the evaluation should\n",
        "        use a stochastic or deterministic policy.\n",
        "    :param verbose:\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        eval_env: gym.Env,\n",
        "        trial: optuna.Trial,\n",
        "        n_eval_episodes: int = 5,\n",
        "        eval_freq: int = 10000,\n",
        "        deterministic: bool = True,\n",
        "        verbose: int = 0,\n",
        "    ):\n",
        "\n",
        "        super().__init__(\n",
        "            eval_env=eval_env,\n",
        "            n_eval_episodes=n_eval_episodes,\n",
        "            eval_freq=eval_freq,\n",
        "            deterministic=deterministic,\n",
        "            verbose=verbose,\n",
        "        )\n",
        "        self.trial = trial\n",
        "        self.eval_idx = 0\n",
        "        self.is_pruned = False\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
        "            # Evaluate policy (done in the parent class)\n",
        "            super()._on_step()\n",
        "            self.eval_idx += 1\n",
        "            # Send report to Optuna\n",
        "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
        "            # Prune trial if need\n",
        "            if self.trial.should_prune():\n",
        "                self.is_pruned = True\n",
        "                return False\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial: optuna.Trial) -> float:\n",
        "    \"\"\"\n",
        "    Objective function using by Optuna to evaluate\n",
        "    one configuration (i.e., one set of hyperparameters).\n",
        "\n",
        "    Given a trial object, it will sample hyperparameters,\n",
        "    evaluate it and report the result (mean episodic reward after training)\n",
        "\n",
        "    :param trial: Optuna trial object\n",
        "    :return: Mean episodic reward after training\n",
        "    \"\"\"\n",
        "\n",
        "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
        "    ### YOUR CODE HERE\n",
        "    # TODO:\n",
        "    # 1. Sample hyperparameters and update the default keyword arguments: `kwargs.update(other_params)`\n",
        "    # 2. Create the evaluation envs\n",
        "    # 3. Create the `TrialEvalCallback`\n",
        "\n",
        "    # 1. Sample hyperparameters and update the keyword arguments\n",
        "    kwargs.update(sample_ppo_params(trial))\n",
        "    # Create the RL model\n",
        "    model = PPO(**kwargs, device=\"cpu\")\n",
        "\n",
        "    # 2. Create envs used for evaluation using `make_vec_env`, `ENV_ID` and `N_EVAL_ENVS`\n",
        "    env = make_vec_env(env_id=ENV_ID, n_envs=N_EVAL_ENVS)\n",
        "    # 3. Create the `TrialEvalCallback` callback defined above that will periodically evaluate\n",
        "    # and report the performance using `N_EVAL_EPISODES` every `EVAL_FREQ`\n",
        "    # TrialEvalCallback signature:\n",
        "    # TrialEvalCallback(eval_env, trial, n_eval_episodes, eval_freq, deterministic, verbose)\n",
        "    eval_callback = TrialEvalCallback(env, trial, N_EVAL_EPISODES, EVAL_FREQ, True, 1)\n",
        "\n",
        "    ### END OF YOUR CODE\n",
        "\n",
        "    nan_encountered = False\n",
        "    try:\n",
        "        # Train the model\n",
        "        model.learn(N_TIMESTEPS, callback=eval_callback)\n",
        "    except AssertionError as e:\n",
        "        # Sometimes, random hyperparams can generate NaN\n",
        "        print(e)\n",
        "        nan_encountered = True\n",
        "    finally:\n",
        "        # Free memory\n",
        "        model.env.close()\n",
        "        env.close()\n",
        "\n",
        "    # Tell the optimizer that the trial failed\n",
        "    if nan_encountered:\n",
        "        return float(\"nan\")\n",
        "\n",
        "    if eval_callback.is_pruned:\n",
        "        raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return eval_callback.last_mean_reward"
      ],
      "metadata": {
        "id": "K3jXU-YEyLdf"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as th\n",
        "\n",
        "# Set pytorch num threads to 1 for faster training\n",
        "th.set_num_threads(1)\n",
        "# Select the sampler, can be random, TPESampler, CMAES, ...\n",
        "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
        "# Do not prune before 1/3 of the max budget is used\n",
        "pruner = MedianPruner(\n",
        "    n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3\n",
        ")\n",
        "# Create the study and start the hyperparameter optimization\n",
        "study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
        "\n",
        "try:\n",
        "    study.optimize(objective, n_trials=N_TRIALS, n_jobs=N_JOBS, timeout=TIMEOUT)\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "\n",
        "print(\"Number of finished trials: \", len(study.trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(f\"  Value: {trial.value}\")\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "print(\"  User attrs:\")\n",
        "for key, value in trial.user_attrs.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "# Write report\n",
        "study.trials_dataframe().to_csv(\"study_results_ppo_cartpole.csv\")\n",
        "\n",
        "fig1 = plot_optimization_history(study)\n",
        "fig2 = plot_param_importances(study)\n",
        "\n",
        "fig1.show()\n",
        "fig2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AVADDC6PyTjP",
        "outputId": "678fa4c3-ad0c-4e3c-8967-74eeba7a3dec"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 08:54:09,772] A new study created in memory with name: no-name-5756be1a-f8bd-444b-a78f-3d70f9f244a2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-233.81 +/- 114.16\n",
            "Episode length: 136.40 +/- 23.50\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-164.40 +/- 53.72\n",
            "Episode length: 687.80 +/- 173.60\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 08:55:11,449] Trial 0 finished with value: -164.3962309 and parameters: {'gamma': 0.000744730179195844, 'max_grad_norm': 2.6637881242995936, 'exponent_n_steps': 10, 'learning_rate': 0.001371402498309818, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 0 with value: -164.3962309.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-573.04 +/- 145.36\n",
            "Episode length: 69.10 +/- 8.61\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-620.98 +/- 189.79\n",
            "Episode length: 73.00 +/- 12.82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 08:55:47,626] Trial 1 finished with value: -620.9750089 and parameters: {'gamma': 0.013998957413102604, 'max_grad_norm': 0.38078976749173177, 'exponent_n_steps': 9, 'learning_rate': 0.5689997431746266, 'net_arch': 'tiny', 'activation_fn': 'tanh'}. Best is trial 0 with value: -164.3962309.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-717.64 +/- 290.08\n",
            "Episode length: 220.00 +/- 31.12\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-719.41 +/- 99.40\n",
            "Episode length: 419.50 +/- 58.95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 08:56:31,724] Trial 2 finished with value: -719.4091194 and parameters: {'gamma': 0.09874241549431143, 'max_grad_norm': 2.3843290080941846, 'exponent_n_steps': 10, 'learning_rate': 0.0006494063071454294, 'net_arch': 'tiny', 'activation_fn': 'tanh'}. Best is trial 0 with value: -164.3962309.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-1082.23 +/- 654.99\n",
            "Episode length: 150.70 +/- 77.73\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-1079.85 +/- 945.32\n",
            "Episode length: 143.90 +/- 87.79\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 08:57:14,043] Trial 3 finished with value: -1079.854274 and parameters: {'gamma': 0.06966806981339066, 'max_grad_norm': 0.3335261115039163, 'exponent_n_steps': 10, 'learning_rate': 0.00012391494181636037, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 0 with value: -164.3962309.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-187.33 +/- 103.93\n",
            "Episode length: 71.80 +/- 14.08\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-236.87 +/- 129.80\n",
            "Episode length: 74.30 +/- 10.23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 08:57:47,884] Trial 4 finished with value: -236.8713959 and parameters: {'gamma': 0.0002990749417515859, 'max_grad_norm': 3.741980229038252, 'exponent_n_steps': 10, 'learning_rate': 0.030720590843176118, 'net_arch': 'tiny', 'activation_fn': 'tanh'}. Best is trial 0 with value: -164.3962309.\n",
            "[I 2025-02-24 08:58:07,903] Trial 5 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-996.50 +/- 704.72\n",
            "Episode length: 139.00 +/- 59.24\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=-199.73 +/- 67.25\n",
            "Episode length: 206.20 +/- 83.89\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-510.56 +/- 111.23\n",
            "Episode length: 97.20 +/- 34.84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 08:58:52,959] Trial 6 finished with value: -510.5634592 and parameters: {'gamma': 0.0019651752094409385, 'max_grad_norm': 4.989548695724667, 'exponent_n_steps': 10, 'learning_rate': 0.01263724271973843, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 0 with value: -164.3962309.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-380.70 +/- 567.52\n",
            "Episode length: 341.50 +/- 197.86\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=68.75 +/- 119.86\n",
            "Episode length: 641.10 +/- 126.07\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:00:01,877] Trial 7 finished with value: 68.74624159999999 and parameters: {'gamma': 0.0001015392328807807, 'max_grad_norm': 1.4345497460836967, 'exponent_n_steps': 9, 'learning_rate': 0.001632956204180654, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 7 with value: 68.74624159999999.\n",
            "[I 2025-02-24 09:00:21,243] Trial 8 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-597.27 +/- 158.84\n",
            "Episode length: 65.30 +/- 8.12\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:00:42,001] Trial 9 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-1208.60 +/- 1440.03\n",
            "Episode length: 163.40 +/- 103.95\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=-137.71 +/- 18.04\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=5.21 +/- 23.79\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:02:10,112] Trial 10 finished with value: 5.2075553 and parameters: {'gamma': 0.00012216001865418427, 'max_grad_norm': 0.6299827124232847, 'exponent_n_steps': 9, 'learning_rate': 0.01816290609152004, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 7 with value: 68.74624159999999.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-72.76 +/- 132.46\n",
            "Episode length: 641.00 +/- 288.94\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-78.73 +/- 120.27\n",
            "Episode length: 576.60 +/- 197.10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:03:18,004] Trial 11 finished with value: -78.72605089999999 and parameters: {'gamma': 0.00011075007939486335, 'max_grad_norm': 0.6209515040345722, 'exponent_n_steps': 9, 'learning_rate': 0.008144493787027416, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 7 with value: 68.74624159999999.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-142.54 +/- 52.04\n",
            "Episode length: 71.00 +/- 10.06\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-159.10 +/- 48.84\n",
            "Episode length: 63.60 +/- 10.36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:03:51,564] Trial 12 finished with value: -159.1001789 and parameters: {'gamma': 0.0011682460570618548, 'max_grad_norm': 1.4287207208997943, 'exponent_n_steps': 9, 'learning_rate': 0.04891700337416687, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 7 with value: 68.74624159999999.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-183.03 +/- 68.22\n",
            "Episode length: 291.40 +/- 97.41\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=32.19 +/- 148.44\n",
            "Episode length: 303.70 +/- 96.35\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:04:40,007] Trial 13 finished with value: 32.193333900000006 and parameters: {'gamma': 0.00024455908145303815, 'max_grad_norm': 0.5840230534191635, 'exponent_n_steps': 9, 'learning_rate': 0.0028286990985151323, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 7 with value: 68.74624159999999.\n",
            "[I 2025-02-24 09:05:05,220] Trial 14 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-337.29 +/- 47.47\n",
            "Episode length: 333.60 +/- 140.73\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:05:22,032] Trial 15 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-453.05 +/- 188.87\n",
            "Episode length: 142.80 +/- 53.57\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=-104.56 +/- 25.88\n",
            "Episode length: 245.90 +/- 203.33\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-182.21 +/- 29.47\n",
            "Episode length: 641.80 +/- 117.35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:06:21,051] Trial 16 finished with value: -182.20978399999998 and parameters: {'gamma': 0.00053556227876498, 'max_grad_norm': 0.9032611029072047, 'exponent_n_steps': 9, 'learning_rate': 0.0038831163570780683, 'net_arch': 'tiny', 'activation_fn': 'tanh'}. Best is trial 7 with value: 68.74624159999999.\n",
            "[I 2025-02-24 09:06:39,724] Trial 17 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-522.20 +/- 155.01\n",
            "Episode length: 61.80 +/- 9.95\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:06:57,550] Trial 18 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-1175.68 +/- 880.31\n",
            "Episode length: 162.60 +/- 72.17\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:07:18,132] Trial 19 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-681.96 +/- 350.79\n",
            "Episode length: 120.30 +/- 45.43\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:07:38,507] Trial 20 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-2034.66 +/- 593.69\n",
            "Episode length: 408.90 +/- 66.49\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:08:04,288] Trial 21 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-194.04 +/- 77.83\n",
            "Episode length: 381.90 +/- 287.68\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:08:33,476] Trial 22 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-241.80 +/- 38.22\n",
            "Episode length: 566.40 +/- 206.42\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=-127.66 +/- 15.19\n",
            "Episode length: 69.80 +/- 11.29\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-119.69 +/- 46.41\n",
            "Episode length: 73.70 +/- 15.09\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:09:07,364] Trial 23 finished with value: -119.69151190000002 and parameters: {'gamma': 0.00010954228261965336, 'max_grad_norm': 1.212662591796925, 'exponent_n_steps': 9, 'learning_rate': 0.11190761327284113, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 7 with value: 68.74624159999999.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-167.61 +/- 26.21\n",
            "Episode length: 367.60 +/- 82.06\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=51.01 +/- 146.69\n",
            "Episode length: 339.10 +/- 52.57\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:10:06,075] Trial 24 finished with value: 51.0077633 and parameters: {'gamma': 0.00038542582586045826, 'max_grad_norm': 0.5095886019626862, 'exponent_n_steps': 9, 'learning_rate': 0.002481673734859857, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 7 with value: 68.74624159999999.\n",
            "[I 2025-02-24 09:10:28,194] Trial 25 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-379.27 +/- 35.27\n",
            "Episode length: 392.80 +/- 104.01\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:10:48,285] Trial 26 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-553.79 +/- 231.80\n",
            "Episode length: 132.90 +/- 27.23\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=-140.40 +/- 19.87\n",
            "Episode length: 266.20 +/- 90.38\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-225.01 +/- 60.00\n",
            "Episode length: 344.90 +/- 258.64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:11:40,482] Trial 27 finished with value: -225.0056054 and parameters: {'gamma': 0.0002121778570645785, 'max_grad_norm': 1.1016878412880655, 'exponent_n_steps': 9, 'learning_rate': 0.004768458787904804, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 7 with value: 68.74624159999999.\n",
            "[I 2025-02-24 09:11:59,377] Trial 28 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-416.66 +/- 146.26\n",
            "Episode length: 111.10 +/- 23.84\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:12:22,423] Trial 29 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-860.43 +/- 121.95\n",
            "Episode length: 365.20 +/- 61.77\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:12:41,241] Trial 30 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-941.53 +/- 280.63\n",
            "Episode length: 229.00 +/- 108.03\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:13:01,259] Trial 31 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-188.46 +/- 48.61\n",
            "Episode length: 254.50 +/- 160.62\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:13:27,674] Trial 32 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-186.15 +/- 70.49\n",
            "Episode length: 431.00 +/- 245.98\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=-26.88 +/- 120.95\n",
            "Episode length: 375.60 +/- 134.07\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=71.22 +/- 117.42\n",
            "Episode length: 374.70 +/- 279.15\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:14:18,451] Trial 33 finished with value: 71.2168073 and parameters: {'gamma': 0.00015930074953660867, 'max_grad_norm': 0.7434341114612586, 'exponent_n_steps': 9, 'learning_rate': 0.015460089685740275, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 33 with value: 71.2168073.\n",
            "[I 2025-02-24 09:14:34,205] Trial 34 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-504.44 +/- 110.28\n",
            "Episode length: 64.10 +/- 11.61\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:14:51,977] Trial 35 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-633.91 +/- 292.68\n",
            "Episode length: 117.70 +/- 30.58\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:15:09,785] Trial 36 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-1096.87 +/- 691.35\n",
            "Episode length: 146.70 +/- 68.31\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:15:28,983] Trial 37 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-805.93 +/- 518.14\n",
            "Episode length: 185.20 +/- 60.46\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:15:50,926] Trial 38 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-345.54 +/- 115.47\n",
            "Episode length: 170.10 +/- 57.24\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=-114.94 +/- 36.55\n",
            "Episode length: 275.20 +/- 88.72\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-145.23 +/- 57.95\n",
            "Episode length: 712.20 +/- 255.37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:16:56,619] Trial 39 finished with value: -145.22774529999998 and parameters: {'gamma': 0.0001560555537176508, 'max_grad_norm': 1.430520934827267, 'exponent_n_steps': 9, 'learning_rate': 0.002361040519629351, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 33 with value: 71.2168073.\n",
            "[I 2025-02-24 09:17:19,296] Trial 40 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-519.32 +/- 166.41\n",
            "Episode length: 453.00 +/- 137.54\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:17:48,831] Trial 41 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-279.15 +/- 42.10\n",
            "Episode length: 715.40 +/- 189.08\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=-113.44 +/- 135.44\n",
            "Episode length: 115.90 +/- 30.30\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-141.17 +/- 47.65\n",
            "Episode length: 301.20 +/- 148.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:18:29,030] Trial 42 finished with value: -141.17487169999998 and parameters: {'gamma': 0.00013833484490335238, 'max_grad_norm': 0.5730127329162963, 'exponent_n_steps': 9, 'learning_rate': 0.023498435089824708, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 33 with value: 71.2168073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-31.92 +/- 71.15\n",
            "Episode length: 386.20 +/- 269.63\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=68.90 +/- 165.47\n",
            "Episode length: 389.30 +/- 200.05\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:19:32,121] Trial 43 finished with value: 68.9018012 and parameters: {'gamma': 0.00027937000999767524, 'max_grad_norm': 2.8935374016817104, 'exponent_n_steps': 9, 'learning_rate': 0.00834030364062998, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 33 with value: 71.2168073.\n",
            "[I 2025-02-24 09:20:00,226] Trial 44 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-243.54 +/- 63.58\n",
            "Episode length: 415.80 +/- 204.71\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:20:21,049] Trial 45 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-292.64 +/- 56.10\n",
            "Episode length: 278.90 +/- 109.37\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:20:41,216] Trial 46 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-1036.87 +/- 634.99\n",
            "Episode length: 143.30 +/- 70.46\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:20:59,577] Trial 47 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-266.52 +/- 35.29\n",
            "Episode length: 154.60 +/- 61.92\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:21:18,414] Trial 48 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-231.60 +/- 34.09\n",
            "Episode length: 221.90 +/- 54.88\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:21:39,027] Trial 49 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-945.08 +/- 567.97\n",
            "Episode length: 127.10 +/- 60.67\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:21:56,371] Trial 50 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-297.42 +/- 142.14\n",
            "Episode length: 130.50 +/- 43.70\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:22:21,976] Trial 51 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-143.64 +/- 153.16\n",
            "Episode length: 393.40 +/- 172.64\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=-67.22 +/- 34.27\n",
            "Episode length: 785.60 +/- 327.97\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-98.31 +/- 22.42\n",
            "Episode length: 1000.00 +/- 0.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:23:43,180] Trial 52 finished with value: -98.30626409999999 and parameters: {'gamma': 0.00024157069977113148, 'max_grad_norm': 0.6000196170718943, 'exponent_n_steps': 9, 'learning_rate': 0.011400313473844204, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 33 with value: 71.2168073.\n",
            "[I 2025-02-24 09:24:14,539] Trial 53 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-237.46 +/- 48.54\n",
            "Episode length: 676.90 +/- 209.87\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:24:32,381] Trial 54 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-261.43 +/- 139.51\n",
            "Episode length: 213.10 +/- 121.08\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:24:50,392] Trial 55 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-604.07 +/- 164.09\n",
            "Episode length: 67.20 +/- 9.39\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:25:06,727] Trial 56 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-416.01 +/- 57.46\n",
            "Episode length: 68.10 +/- 11.06\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:25:27,212] Trial 57 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-760.14 +/- 287.63\n",
            "Episode length: 147.10 +/- 21.55\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:25:44,966] Trial 58 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-395.06 +/- 237.62\n",
            "Episode length: 199.20 +/- 101.52\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=-102.12 +/- 21.40\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=49.44 +/- 154.76\n",
            "Episode length: 362.80 +/- 98.14\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:26:50,141] Trial 59 finished with value: 49.44472149999999 and parameters: {'gamma': 0.0002224615420247989, 'max_grad_norm': 0.30153638254046156, 'exponent_n_steps': 9, 'learning_rate': 0.005647088309158515, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 33 with value: 71.2168073.\n",
            "[I 2025-02-24 09:27:35,578] Trial 60 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-158.19 +/- 28.49\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=-61.20 +/- 53.75\n",
            "Episode length: 916.60 +/- 250.20\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=123.36 +/- 54.66\n",
            "Episode length: 872.00 +/- 215.96\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:28:59,587] Trial 61 finished with value: 123.3634229 and parameters: {'gamma': 0.00022750556994400318, 'max_grad_norm': 0.30132099146419916, 'exponent_n_steps': 9, 'learning_rate': 0.0017814207192177304, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 61 with value: 123.3634229.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-77.22 +/- 26.79\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-74.75 +/- 79.09\n",
            "Episode length: 420.80 +/- 157.65\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:30:22,339] Trial 62 finished with value: -74.7539611 and parameters: {'gamma': 0.0002451893310930031, 'max_grad_norm': 0.3085971231112136, 'exponent_n_steps': 9, 'learning_rate': 0.0019459184585463043, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 61 with value: 123.3634229.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-68.52 +/- 16.46\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=171.74 +/- 73.93\n",
            "Episode length: 412.00 +/- 104.42\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:31:30,545] Trial 63 finished with value: 171.7401334 and parameters: {'gamma': 0.00017308880670360112, 'max_grad_norm': 0.35278706193345905, 'exponent_n_steps': 9, 'learning_rate': 0.0029920996171534323, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 63 with value: 171.7401334.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-66.30 +/- 21.22\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=153.66 +/- 40.65\n",
            "Episode length: 518.70 +/- 173.26\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:32:53,894] Trial 64 finished with value: 153.66170960000002 and parameters: {'gamma': 0.00018255936363708585, 'max_grad_norm': 0.3614459029416312, 'exponent_n_steps': 9, 'learning_rate': 0.0006867286581962038, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 63 with value: 171.7401334.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-101.88 +/- 158.75\n",
            "Episode length: 905.30 +/- 197.29\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:34:09,603] Trial 65 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=20000, episode_reward=-183.96 +/- 76.61\n",
            "Episode length: 848.80 +/- 199.90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:34:27,983] Trial 66 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-129.24 +/- 50.28\n",
            "Episode length: 69.50 +/- 7.77\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:34:49,908] Trial 67 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-1523.68 +/- 424.14\n",
            "Episode length: 276.30 +/- 100.04\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:35:10,018] Trial 68 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-882.75 +/- 470.97\n",
            "Episode length: 212.40 +/- 88.23\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:35:33,010] Trial 69 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-199.86 +/- 35.40\n",
            "Episode length: 364.90 +/- 148.17\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:35:52,404] Trial 70 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-142.18 +/- 26.96\n",
            "Episode length: 70.70 +/- 12.60\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=-127.02 +/- 25.46\n",
            "Episode length: 70.70 +/- 12.63\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:36:33,498] Trial 71 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=20000, episode_reward=-292.22 +/- 60.71\n",
            "Episode length: 121.60 +/- 43.26\n",
            "Eval num_timesteps=10000, episode_reward=-9.82 +/- 102.34\n",
            "Episode length: 777.60 +/- 164.82\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=104.47 +/- 104.48\n",
            "Episode length: 524.50 +/- 214.45\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:37:50,385] Trial 72 finished with value: 104.4664837 and parameters: {'gamma': 0.00023239237110234427, 'max_grad_norm': 0.38065782933836734, 'exponent_n_steps': 9, 'learning_rate': 0.0014493137471918926, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 63 with value: 171.7401334.\n",
            "[I 2025-02-24 09:38:19,542] Trial 73 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-209.51 +/- 92.95\n",
            "Episode length: 568.70 +/- 263.07\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:38:44,872] Trial 74 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-1195.28 +/- 606.80\n",
            "Episode length: 334.00 +/- 171.52\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:39:05,298] Trial 75 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-1173.11 +/- 650.18\n",
            "Episode length: 166.10 +/- 68.67\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=-69.95 +/- 28.41\n",
            "Episode length: 197.70 +/- 38.10\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=11.87 +/- 105.89\n",
            "Episode length: 338.40 +/- 234.38\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:39:54,405] Trial 76 finished with value: 11.8655344 and parameters: {'gamma': 0.00016286308713733594, 'max_grad_norm': 0.3321146036645639, 'exponent_n_steps': 9, 'learning_rate': 0.0032676804068712917, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 63 with value: 171.7401334.\n",
            "[I 2025-02-24 09:40:18,949] Trial 77 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-1725.95 +/- 685.70\n",
            "Episode length: 432.80 +/- 49.68\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:40:37,650] Trial 78 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-127.72 +/- 28.16\n",
            "Episode length: 88.70 +/- 41.64\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=-37.27 +/- 152.49\n",
            "Episode length: 219.60 +/- 62.70\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=34.62 +/- 112.40\n",
            "Episode length: 230.60 +/- 117.46\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:41:27,916] Trial 79 finished with value: 34.6213547 and parameters: {'gamma': 0.00019846471598402272, 'max_grad_norm': 1.5780864529232235, 'exponent_n_steps': 9, 'learning_rate': 0.004087373983683397, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 63 with value: 171.7401334.\n",
            "[I 2025-02-24 09:41:56,800] Trial 80 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-143.44 +/- 75.79\n",
            "Episode length: 334.10 +/- 308.45\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:42:20,350] Trial 81 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-128.63 +/- 22.53\n",
            "Episode length: 313.10 +/- 135.62\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:42:46,102] Trial 82 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-150.31 +/- 17.53\n",
            "Episode length: 522.80 +/- 68.77\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:43:25,424] Trial 83 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-127.17 +/- 61.73\n",
            "Episode length: 922.00 +/- 116.82\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:43:45,989] Trial 84 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-461.17 +/- 54.19\n",
            "Episode length: 191.70 +/- 40.20\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=-95.84 +/- 129.34\n",
            "Episode length: 601.40 +/- 196.07\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:44:50,274] Trial 85 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=20000, episode_reward=-165.96 +/- 14.77\n",
            "Episode length: 360.20 +/- 91.97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:45:26,717] Trial 86 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-240.89 +/- 102.19\n",
            "Episode length: 641.60 +/- 256.39\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=-114.06 +/- 22.53\n",
            "Episode length: 272.10 +/- 75.48\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:46:10,384] Trial 87 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=20000, episode_reward=-287.71 +/- 53.60\n",
            "Episode length: 171.20 +/- 29.76\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:46:53,134] Trial 88 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-172.86 +/- 13.06\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:47:18,211] Trial 89 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-709.32 +/- 283.10\n",
            "Episode length: 362.80 +/- 196.62\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:47:38,405] Trial 90 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-470.72 +/- 106.23\n",
            "Episode length: 128.80 +/- 44.89\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:47:59,500] Trial 91 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-121.15 +/- 48.86\n",
            "Episode length: 201.00 +/- 70.72\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:48:21,419] Trial 92 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-148.04 +/- 25.37\n",
            "Episode length: 283.00 +/- 73.02\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:48:46,253] Trial 93 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-193.97 +/- 104.07\n",
            "Episode length: 358.60 +/- 85.46\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=-107.62 +/- 47.54\n",
            "Episode length: 538.90 +/- 244.38\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=86.52 +/- 112.81\n",
            "Episode length: 601.70 +/- 209.53\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:49:58,443] Trial 94 finished with value: 86.5234044 and parameters: {'gamma': 0.00024644610863116726, 'max_grad_norm': 1.813546334015925, 'exponent_n_steps': 9, 'learning_rate': 0.0013359659036871613, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 63 with value: 171.7401334.\n",
            "[I 2025-02-24 09:50:20,436] Trial 95 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-135.32 +/- 19.48\n",
            "Episode length: 258.20 +/- 107.65\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:50:47,510] Trial 96 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-466.48 +/- 68.56\n",
            "Episode length: 645.40 +/- 132.48\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:51:29,035] Trial 97 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-231.32 +/- 69.13\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:52:05,442] Trial 98 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-453.63 +/- 101.27\n",
            "Episode length: 937.60 +/- 130.08\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=-55.76 +/- 100.98\n",
            "Episode length: 927.30 +/- 148.23\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-153.97 +/- 39.95\n",
            "Episode length: 258.70 +/- 106.76\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 09:53:06,894] Trial 99 finished with value: -153.9720122 and parameters: {'gamma': 0.0003490546304377089, 'max_grad_norm': 1.223917011760367, 'exponent_n_steps': 10, 'learning_rate': 0.0006083387429401111, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 63 with value: 171.7401334.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of finished trials:  100\n",
            "Best trial:\n",
            "  Value: 171.7401334\n",
            "  Params: \n",
            "    gamma: 0.00017308880670360112\n",
            "    max_grad_norm: 0.35278706193345905\n",
            "    exponent_n_steps: 9\n",
            "    learning_rate: 0.0029920996171534323\n",
            "    net_arch: small\n",
            "    activation_fn: relu\n",
            "  User attrs:\n",
            "    gamma_: 0.9998269111932964\n",
            "    n_steps: 512\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"9e03c6d0-c12c-4a8c-af72-9954085abfd1\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9e03c6d0-c12c-4a8c-af72-9954085abfd1\")) {                    Plotly.newPlot(                        \"9e03c6d0-c12c-4a8c-af72-9954085abfd1\",                        [{\"mode\":\"markers\",\"name\":\"Objective Value\",\"x\":[0,1,2,3,4,6,7,10,11,12,13,16,23,24,27,33,39,42,43,52,59,61,62,63,64,72,76,79,94,99],\"y\":[-164.3962309,-620.9750089,-719.4091194,-1079.854274,-236.8713959,-510.5634592,68.74624159999999,5.2075553,-78.72605089999999,-159.1001789,32.193333900000006,-182.20978399999998,-119.69151190000002,51.0077633,-225.0056054,71.2168073,-145.22774529999998,-141.17487169999998,68.9018012,-98.30626409999999,49.44472149999999,123.3634229,-74.7539611,171.7401334,153.66170960000002,104.4664837,11.8655344,34.6213547,86.5234044,-153.9720122],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Best Value\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],\"y\":[-164.3962309,-164.3962309,-164.3962309,-164.3962309,-164.3962309,-164.3962309,-164.3962309,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,68.74624159999999,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,71.2168073,123.3634229,123.3634229,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334,171.7401334],\"type\":\"scatter\"},{\"marker\":{\"color\":\"#cccccc\"},\"mode\":\"markers\",\"name\":\"Infeasible Trial\",\"showlegend\":false,\"x\":[],\"y\":[],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Optimization History Plot\"},\"xaxis\":{\"title\":{\"text\":\"Trial\"}},\"yaxis\":{\"title\":{\"text\":\"Objective Value\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9e03c6d0-c12c-4a8c-af72-9954085abfd1');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"c617e90a-6b9c-4e5b-964d-2bac982ffaa2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c617e90a-6b9c-4e5b-964d-2bac982ffaa2\")) {                    Plotly.newPlot(                        \"c617e90a-6b9c-4e5b-964d-2bac982ffaa2\",                        [{\"cliponaxis\":false,\"hovertemplate\":[\"exponent_n_steps (IntDistribution): 0.019880132718273835\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"learning_rate (FloatDistribution): 0.06954376336965505\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"net_arch (CategoricalDistribution): 0.10704426927257435\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"max_grad_norm (FloatDistribution): 0.12692010025104422\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"activation_fn (CategoricalDistribution): 0.18804155774102657\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"gamma (FloatDistribution): 0.4885701766474259\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"],\"name\":\"Objective Value\",\"orientation\":\"h\",\"text\":[\"0.02\",\"0.07\",\"0.11\",\"0.13\",\"0.19\",\"0.49\"],\"textposition\":\"outside\",\"x\":[0.019880132718273835,0.06954376336965505,0.10704426927257435,0.12692010025104422,0.18804155774102657,0.4885701766474259],\"y\":[\"exponent_n_steps\",\"learning_rate\",\"net_arch\",\"max_grad_norm\",\"activation_fn\",\"gamma\"],\"type\":\"bar\"}],                        {\"title\":{\"text\":\"Hyperparameter Importances\"},\"xaxis\":{\"title\":{\"text\":\"Hyperparameter Importance\"}},\"yaxis\":{\"title\":{\"text\":\"Hyperparameter\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c617e90a-6b9c-4e5b-964d-2bac982ffaa2');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAO8RerQxLta"
      },
      "outputs": [],
      "source": [
        "\n",
        "#env = gym.make(\"LunarLander-v3\")\n",
        "policy_kwargs={\n",
        "            \"net_arch\": {\"pi\": [64, 64], \"vf\": [64, 64]},\n",
        "            \"activation_fn\": nn.ReLU,\n",
        "        }\n",
        "env = make_vec_env(\"LunarLander-v3\", n_envs=5)\n",
        "model = PPO(\n",
        "    policy=\"MlpPolicy\",\n",
        "    env=env,\n",
        "    n_steps=512,\n",
        "    batch_size=64,\n",
        "    n_epochs=4,\n",
        "    gamma=0.9998269111932964,\n",
        "    max_grad_norm= 0.35278706193345905,\n",
        "    policy_kwargs=policy_kwargs,\n",
        "    learning_rate= 0.0029920996171534323\n",
        ")\n",
        "\n",
        "model.learn(total_timesteps=300000)\n",
        "# Save the model\n",
        "model_name = \"ppo-LunarLander-v3\"\n",
        "model.save(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfFfD-iMxLta"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
        "model = PPO.load(\"results/LunarLander/ppo-LunarLander-v3.zip\")\n",
        "\n",
        "obs, info = env.reset()\n",
        "while True:\n",
        "    action, _states = model.predict(obs)\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    if terminated or truncated:\n",
        "        break\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}